{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extract Features'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "def extract_X():\n",
    "\n",
    "    file_path = r\"\"\n",
    "\n",
    "    Columns=[]\n",
    "    # C-J:data of sampling points at different depths\n",
    "    for i in range(16):\n",
    "        df_specific_sheet = pd.read_excel(file_path, sheet_name=i+1)\n",
    "        # using sheet index (the first sheet serves as the table of contents)\n",
    "        for c in range(8):\n",
    "            column=df_specific_sheet.iloc[:, 2+c][1:]\n",
    "            Columns.append(column.values)\n",
    "\n",
    "    samples=[]\n",
    "    for i in range(49):\n",
    "        \n",
    "        samples.append(i)\n",
    "    X=[]\n",
    "    for sample in samples:\n",
    "        for depth in range(8):\n",
    "            features=[]\n",
    "            # print(depth)\n",
    "            features.append(depth*10)\n",
    "            for f in range(16):\n",
    "                val=Columns[f*8+depth][sample]\n",
    "                features.append(val)\n",
    "\n",
    "            X.append(features)\n",
    "    X=np.array(X)\n",
    "    return X\n",
    "X=extract_X()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Extract Labels'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "def extract_Y():\n",
    "    # Reading data\n",
    "    file_path = r\"\"\n",
    "    Columns=[]\n",
    "\n",
    "    df_specific_sheet = pd.read_excel(file_path, sheet_name=\"Total UVAs\") \n",
    "    for c in range(8):\n",
    "        column=df_specific_sheet.iloc[:, 2+c][1:]\n",
    "        Columns.append(column.values)\n",
    "\n",
    "    Y=[]\n",
    "\n",
    "    samples=[]\n",
    "    for i in range(49):\n",
    "        \n",
    "        samples.append(i)\n",
    "    for sample in samples:\n",
    "        \n",
    "        for depth in range(8):\n",
    "            # 8列\n",
    "            labels=[]\n",
    "\n",
    "            for f in range(1):\n",
    "                # f是1个sheet\n",
    "                # 一个文件有8个cols\n",
    "                val=Columns[f*8+depth][sample]\n",
    "                labels.append(val)\n",
    "\n",
    "            Y.append(labels)\n",
    "                \n",
    "    Y=np.array(Y)\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = [\n",
    "#     \"Depth\",  #0      # depth\n",
    "#     \"Temp\",   #1      # Temperature  \n",
    "#     \"Sal\",    #2      # Salinity\n",
    "#     \"HFVel\",  #3      # Horizontal Flow Velocity\n",
    "#     \"VFVel\",  #4      # Vertical Flow Velocity\n",
    "#     \"DO\",     #5      # Dissolved Oxygen\n",
    "#     \"DIC\",    #6      # Dissolved Inorganic Carbon\n",
    "#     \"NO3\",    #7      # Nitrate\n",
    "#     \"PO4\",    #8      # Phosphate\n",
    "#     \"SiO4\",   #9      # Silicate\n",
    "#     \"Fe2+\",   #10      # Dissolved Iron Ions\n",
    "#     \"Chl\",    #11      # Chlorophyll\n",
    "#     \"EHS\",    #12\n",
    "#     \"OD-PABA\",  #13\n",
    "#     \"EHMC\",     #14 \n",
    "#     \"OC\",       #15\n",
    "\n",
    "# ]\n",
    "def filter_data(X,Y):\n",
    "    \"\"\"\n",
    "    filter_data: kill nonce data\n",
    "    \"\"\"\n",
    "    Train_X=[]\n",
    "    Train_Y=[]\n",
    "    for i in range(len(Y)):\n",
    "        label=0\n",
    "        for m in range(X.shape[1]):\n",
    "            if np.isnan(X[i][m]):\n",
    "                label=1\n",
    "                break\n",
    "        if np.isnan(Y[i][0]) !=1 and label==0 and Y[i][0]<1000:\n",
    "            Train_X.append(X[i])\n",
    "            Train_Y.append(Y[i])\n",
    "\n",
    "    X_Train=np.array(Train_X)\n",
    "    Y_Train=np.array(Train_Y)\n",
    "    cols=[]\n",
    "    X_Train = np.delete(X_Train, cols, axis=1)\n",
    "    print(\"Check shape:\")\n",
    "    print(X_Train.shape,Y_Train.shape)\n",
    "    return X_Train,Y_Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import *\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.regularizers import *\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResDual_mlp_model(input_size:int, learning_rate=0.004, key_feature_idx=-4):\n",
    "    inputs = Input(shape=(input_size,))\n",
    "    \n",
    "    \n",
    "    key_feature = inputs[:, key_feature_idx:]\n",
    "    \n",
    "    # channel 1\n",
    "    x_key_main = Dense(4, kernel_regularizer=l2(0.01), name='key_fc1', activation=None,kernel_initializer='he_normal')(key_feature)\n",
    "    x_key_main = BatchNormalization()(x_key_main)\n",
    "    x_key_main = Dropout(0.05)(x_key_main)\n",
    "\n",
    "    x_key_main = Dense(abs(key_feature_idx), kernel_regularizer=l2(0.01), activation=None,kernel_initializer='he_normal')(x_key_main)\n",
    "    x_key_main = BatchNormalization()(x_key_main)\n",
    "    x_key_main = Dropout(0.05)(x_key_main)\n",
    "\n",
    "    x_key = Add()([x_key_main, key_feature])\n",
    "    x_key = Activation('relu')(x_key)\n",
    "    x_key = BatchNormalization()(x_key)\n",
    "    x_key_main = Dropout(0.05)(x_key_main)\n",
    "\n",
    "\n",
    "    # channel 2\n",
    "    other_features = inputs[:, :key_feature_idx]\n",
    "    x_other = Dense(8, activation='relu', kernel_regularizer=l2(0.05), name='other_fc1',kernel_initializer='he_normal')(other_features)\n",
    "    x_other = BatchNormalization()(x_other)\n",
    "    x_other = Dropout(0.05)(x_other)\n",
    "\n",
    "    x_other = Dense(8, activation='relu', kernel_regularizer=l2(0.05), name='other_fc2',kernel_initializer='he_normal')(x_other)\n",
    "    x_other = BatchNormalization()(x_other)\n",
    "    x_other = Dropout(0.05)(x_other)\n",
    "\n",
    "\n",
    "    # Merge\n",
    "    merged = Concatenate()([x_key, x_other])\n",
    "    x = Dense(128, activation='relu', name='merged_fc1',kernel_initializer='he_normal')(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    # 256 units is also acceptible\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate, clipnorm=2.0)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# can use optuna for further optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "X=extract_X()\n",
    "Y=extract_Y()\n",
    "\n",
    "X_all,Y_all=filter_data(X,Y)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "fold_histories = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_mae': [], 'val_mae': [],\n",
    "    'test_mae': [], 'test_rmse': []\n",
    "}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "data_size = len(X_all)\n",
    "train_size = int(data_size * 0.8)  \n",
    "\n",
    "\n",
    "import random\n",
    "r=random.randint(0,256)\n",
    "random.seed(42)\n",
    "start_index = random.randint(0, data_size - train_size + 1)\n",
    "\n",
    "\n",
    "X_Train, X_Test, y_Train, Y_test = train_test_split(X_all, Y_all, test_size=0.2, random_state=22)\n",
    "scaler = StandardScaler()\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X_Train)):\n",
    "    \n",
    "    y_scaler = StandardScaler()\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(np.log1p(X_Train[train_idx]))\n",
    "    X_test = scaler.transform(np.log1p(X_Train[test_idx]))\n",
    "\n",
    "    y_train = y_scaler.fit_transform(np.log1p(y_Train[train_idx]).reshape(-1, 1)).flatten()\n",
    "    y_test = y_scaler.fit_transform(np.log1p(y_Train[test_idx]).reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "\n",
    "    model = ResDual_mlp_model(16,key_feature_idx=-4)\n",
    "\n",
    "    X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(\n",
    "        X_train, y_train, test_size=0.15, random_state=42\n",
    "    )\n",
    "    \n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        validation_data=(X_val_fold, y_val_fold),\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "  \n",
    "    best_epoch = np.argmin(history.history['val_loss'])\n",
    "    fold_histories['train_loss'].append(history.history['loss'][best_epoch])\n",
    "    fold_histories['val_loss'].append(history.history['val_loss'][best_epoch])\n",
    "    fold_histories['train_mae'].append(history.history['mae'][best_epoch])\n",
    "    fold_histories['val_mae'].append(history.history['val_mae'][best_epoch])\n",
    "    \n",
    "\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    fold_histories['test_mae'].append(mean_absolute_error(y_test, y_pred))\n",
    "    fold_histories['test_rmse'].append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "\n",
    "    plt.plot(history.history['val_mae'], alpha=0.3, linestyle='--', color=plt.cm.tab10(fold))\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "report = {\n",
    "    'train_loss': f\"{np.mean(fold_histories['train_loss']):.4f} ± {np.std(fold_histories['train_loss']):.4f}\",\n",
    "    'val_loss': f\"{np.mean(fold_histories['val_loss']):.4f} ± {np.std(fold_histories['val_loss']):.4f}\",\n",
    "    'train_mae': f\"{np.mean(fold_histories['train_mae']):.4f} ± {np.std(fold_histories['train_mae']):.4f}\",\n",
    "    'val_mae': f\"{np.mean(fold_histories['val_mae']):.4f} ± {np.std(fold_histories['val_mae']):.4f}\",\n",
    "    'test_mae': f\"{np.mean(fold_histories['test_mae']):.4f} ± {np.std(fold_histories['test_mae']):.4f}\",\n",
    "    'test_rmse': f\"{np.mean(fold_histories['test_rmse']):.4f} ± {np.std(fold_histories['test_rmse']):.4f}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test=scaler.transform(np.log1p(X_Test))\n",
    "y_pred=np.expm1(y_scaler.inverse_transform(model.predict(X_Test)))\n",
    "y_test=Y_test\n",
    "print(\"Predict: \", list(np.round(y_pred.flatten(), 4)))\n",
    "\n",
    "print(\"TrueVal: \", list(np.round(y_test.flatten(), 4)))\n",
    "\n",
    "y_pred=y_pred.reshape(y_pred.shape[0],y_pred.shape[1])\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "rho, p_value = spearmanr(y_pred.flatten(), y_test.flatten())\n",
    "\n",
    "print(f\"Spearman's rank correlation coefficient: {rho}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_test.flatten())\n",
    "\n",
    "\n",
    "print(f\"pearsonr correlation coefficient: {correlation:.4f}\")\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "# 计算 R^2\n",
    "r2 = r2_score(Y_test, y_pred)\n",
    "# 打印 R^2\n",
    "print(f\"R_squared (R^2): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "X_all = scaler.transform(X_all)\n",
    "\n",
    "explainer = shap.DeepExplainer(model, X_train[:])  \n",
    "shap_values = explainer.shap_values(X_train)  \n",
    "\n",
    "feature_names = [\n",
    "    \"Depth\",  #0      # depth\n",
    "    \"Temp\",   #1      # Temperature  \n",
    "    \"Sal\",    #2      # Salinity\n",
    "    \"HFVel\",  #3      # Horizontal Flow Velocity\n",
    "    \"VFVel\",  #4      # Vertical Flow Velocity\n",
    "    \"DO\",     #5      # Dissolved Oxygen\n",
    "    \"DIC\",    #6      # Dissolved Inorganic Carbon\n",
    "    \"NO3\",    #7      # Nitrate\n",
    "    \"PO4\",    #8      # Phosphate\n",
    "    \"SiO4\",   #9      # Silicate\n",
    "    \"Fe2+\",   #10      # Dissolved Iron Ions\n",
    "    \"Chl\",    #11      # Chlorophyll\n",
    "    \"EHS\",    #12\n",
    "    \"OD-PABA\",  #13\n",
    "    \"EHMC\",     #14 \n",
    "    \"OC\",       #15\n",
    "\n",
    "]\n",
    "\n",
    "feature_importance = np.mean(np.abs(shap_values), axis=0).flatten()\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "\n",
    "sorted_importance = feature_importance[sorted_idx]\n",
    "print(\"SHAP:\",sorted_importance)\n",
    "sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "plt.figure(figsize=(12, 8))  \n",
    "colors = plt.cm.viridis(sorted_importance / max(sorted_importance)) \n",
    "bars = plt.bar(range(len(sorted_importance)), sorted_importance, align='center', color=colors)\n",
    "\n",
    "for i, v in enumerate(sorted_importance):\n",
    "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.xticks(range(len(sorted_importance)), sorted_names, rotation=45, fontsize=15)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Mean |SHAP Value|\")\n",
    "plt.title(\"SHAP\")\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
